{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Milestone 1: Data Preparation\n","\n","---\n","\n"],"metadata":{"id":"w-U3-TcTXVQw"}},{"cell_type":"markdown","source":["Installing the necessary modules"],"metadata":{"id":"dOir8N8ytRkd"}},{"cell_type":"code","source":["!pip install pytorch-lightning --quiet\n","!pip install wandb --quiet\n","!pip install kagglehub --quiet"],"metadata":{"id":"Jo9rQTv6bfAf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Importing the necessary libs"],"metadata":{"id":"eRaYbbCQcwHd"}},{"cell_type":"code","source":["import pytorch_lightning as pl\n","import torch.nn.functional as F\n","import pytorch_lightning as pl\n","from torchmetrics.classification import Accuracy\n","from torch.utils.data import Dataset, DataLoader, Subset, WeightedRandomSampler\n","from torchvision import transforms\n","from torchvision.datasets import ImageFolder\n","from torch import nn\n","from torch.nn import functional as F\n","from torch.utils.data import DataLoader,random_split\n","from torchmetrics import Accuracy\n","import matplotlib.pyplot as plt\n","import os\n","from PIL import Image\n","import kagglehub\n","import cv2\n","from torch.utils.data import DataLoader, random_split\n","from pytorch_lightning.loggers import WandbLogger\n","import wandb\n","from sklearn.utils.class_weight import compute_class_weight\n","import torch.optim as optim\n","import torch\n","\n","import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.model_selection import StratifiedKFold"],"metadata":{"id":"cYT9tO9Ra8JC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Downloading the Alzheimer's dataset from Kaggle (https://www.kaggle.com/datasets/marcopinamonti/alzheimer-mri-4-classes-dataset?resource=download)\n","\n","And plotting one image for sanity check"],"metadata":{"id":"zuDlmHCV6KhM"}},{"cell_type":"code","source":["# Download the dataset\n","alzheimer_dataset = kagglehub.dataset_download(\"marcopinamonti/alzheimer-mri-4-classes-dataset\")\n","\n","# Obtaining the root folder path\n","root_path = os.path.join(alzheimer_dataset, os.listdir(alzheimer_dataset)[0])\n","\n","# Obtaining the class list\n","class_list = os.listdir(root_path)\n","\n","# Plotting one image of the first class for checking\n","class_path = os.path.join(root_path, class_list[0])\n","image = os.path.join(class_path, \"1 (2).jpg\")\n","plt.imshow(cv2.cvtColor(cv2.imread(image), cv2.COLOR_BGR2RGB))"],"metadata":{"id":"cgMP1AgjbTT6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Organizes and labels each image into its specific class and prepares data for the stratified K-Fold, which was chosen due to the unbalanced nature of the dataset"],"metadata":{"id":"jPJ2qalnNQlR"}},{"cell_type":"code","source":["# Creating the rows of data by matching images with their labels\n","rows = []\n","for label in sorted(class_list):\n","    label_dir = os.path.join(root_path, label)\n","    for img in os.listdir(label_dir):\n","        rows.append({\"class_path\": os.path.join(label_dir, img), \"label\": label})\n","\n","# Creating the data frame with sorted values by label, and reseting the index\n","df = pd.DataFrame(rows).sort_values(\"label\").reset_index(drop=True)\n","\n","# Encode labels 0..3 and converts the names of the classes into numbers\n","le = LabelEncoder()\n","df[\"y\"] = le.fit_transform(df[\"label\"])\n","\n","print(\"Class â†’ index:\", dict(zip(le.classes_, range(len(le.classes_)))))"],"metadata":{"id":"V__kNRtnK4-B"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Making Stratified K-Fold , which ensures that every fold has the same ratio of classes as the full dataset to make a fairer training and validation division."],"metadata":{"id":"OiLN7APZNMXU"}},{"cell_type":"code","source":["# We chose 3 folds\n","K = 3\n","X_idx = np.arange(len(df))\n","y = df[\"y\"].to_numpy()\n","\n","# Generating the 3 folds\n","skf = StratifiedKFold(n_splits=K, shuffle=True, random_state=70)\n","\n","# Storing the validation and training indices for each fold\n","fold_indices = [(X_idx[tr], X_idx[va]) for tr, va in skf.split(X_idx, y)]\n","print(f\"Prepared {len(fold_indices)} folds.\")\n","\n"],"metadata":{"id":"7bTKC3_yNLih"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Since our dataset is very unbalanced and we don't have enough images to test properly, we decided to divide the data into only training and validation sets. Below is the visualization of the 3 generated folds.\n","\n"],"metadata":{"id":"1wTQpqE_PIYK"}},{"cell_type":"code","source":["def counts(arr):\n","    return dict(zip(le.classes_, np.bincount(arr, minlength=len(le.classes_))))\n","\n","for i, (tr, va) in enumerate(fold_indices, start=1):\n","    print(f\"\\n=== Fold {i} ===\")\n","    print(\"Train class counts:\", counts(y[tr]))\n","    print(\"Val class counts:  \", counts(y[va]))\n"],"metadata":{"id":"c56VM-cXPJqg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# ---- Small dataset that reads from your df ----\n","class AlzheimerDataset(Dataset):\n","    def __init__(self, df, transform=None):\n","        self.df = df.reset_index(drop=True)\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):     # Every time the dataloader gets a new image, it will access the dataframe with iloc and then this image will be transformed\n","        row = self.df.iloc[idx]\n","        img = Image.open(row[\"class_path\"]).convert(\"RGB\")\n","        if self.transform:\n","            img = self.transform(img)\n","        label = int(row[\"y\"])\n","        return img, label     # The dataset will return the image and its label\n","\n","\n","class AlzheimerDataModule(pl.LightningDataModule):\n","    def __init__(\n","        self,\n","        df,\n","        fold_indices,\n","        fold_id=0,\n","        batch_size=32,\n","        use_weighted_sampler=True     # Due to the unbalanced dataset\n","    ):\n","        super().__init__()\n","        self.df = df.reset_index(drop=True)\n","        self.fold_indices = fold_indices\n","        self.fold_id = int(fold_id)\n","        self.batch_size = batch_size\n","        self.use_weighted_sampler = use_weighted_sampler\n","\n","        # Makes sure all images are same size (128x128), transforms them to tensors and normalizes\n","        self.transform = transforms.Compose([\n","            transforms.Resize((128, 128)),\n","            transforms.ToTensor(),\n","            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n","        ])\n","\n","        # Gets the number of classes\n","        self.num_classes = int(self.df[\"y\"].nunique())\n","\n","    def setup(self, stage=None):\n","        train_idx, val_idx = self.fold_indices[self.fold_id]\n","\n","        # Base dataset from df\n","        base_ds = AlzheimerDataset(self.df, transform=self.transform)\n","\n","        # Creates a smaller dataset from the base dataset using our obtained training and validation indexes\n","        self.train_data = Subset(base_ds, train_idx)\n","        self.val_data   = Subset(base_ds, val_idx)\n","\n","        # Computes the weights for each class based on the number of images, so the classes with less images have higher weight values\n","        train_labels = self.df.iloc[train_idx][\"y\"].to_numpy()\n","        classes = np.arange(self.num_classes)\n","        cw = compute_class_weight(class_weight='balanced', classes=classes, y=train_labels)\n","\n","        # Increases the punishment of the model in case it fails to correctly label the class with less images, since we have less data to train on it\n","        self.class_weights = torch.tensor(cw, dtype=torch.float32)\n","        print(f\"Class Weights: {self.class_weights.tolist()}\")\n","\n","        # We are applying a weighted sampler to increase the loading of the class with less images to try and make the training more balanced\n","        if self.use_weighted_sampler:\n","            class_counts = np.bincount(train_labels, minlength=self.num_classes)\n","            per_class_w = 1.0 / class_counts\n","            sample_w = per_class_w[train_labels]\n","            self.train_sampler = WeightedRandomSampler(\n","                weights=torch.tensor(sample_w, dtype=torch.double),\n","                num_samples=len(sample_w),\n","                replacement=True\n","            )\n","        else:\n","            self.train_sampler = None\n","\n","\n","    def train_dataloader(self):\n","        return DataLoader(\n","            self.train_data,\n","            batch_size=self.batch_size,\n","            shuffle=(self.train_sampler is None),     # In case there is no training sampler, we would shuffle the data\n","            sampler=self.train_sampler,               # However we are using a weighted sampler to try and increase the balance of the classes\n","            pin_memory=True                           # Optimizing the data transfer\n","        )\n","\n","    def val_dataloader(self):\n","        return DataLoader(\n","            self.val_data,\n","            batch_size=self.batch_size,\n","            shuffle=False,\n","            pin_memory=True\n","        )"],"metadata":{"id":"sN902vohovci"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class AlzheimerLitModel(pl.LightningModule):\n","    def __init__(self, input_shape, num_classes, learning_rate=3e-4, class_weights=None):\n","        super().__init__()\n","        # save_hyperparameters saves the __init__ args automatically\n","        self.save_hyperparameters()\n","\n","        self.input_shape = input_shape\n","        self.learning_rate = learning_rate\n","\n","        # Feature extractor (convolutional layers)\n","        in_channels = input_shape[0]\n","        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=32, kernel_size=3, stride=1, padding=1)\n","        self.conv2 = nn.Conv2d(32, 32, 3, 1, padding=1)\n","        self.conv3 = nn.Conv2d(32, 64, 3, 1, padding=1)\n","        self.conv4 = nn.Conv2d(64, 64, 3, 1, padding=1)\n","        self.pool1 = nn.MaxPool2d(2)\n","        self.pool2 = nn.MaxPool2d(2)\n","\n","        # Compute flattened feature size after convs\n","        n_sizes = self._get_output_shape(input_shape)\n","\n","        # Classifier\n","        self.fc1 = nn.Linear(n_sizes, 512)\n","        self.fc2 = nn.Linear(512, 128)\n","        self.fc3 = nn.Linear(128, num_classes)\n","\n","        # If class_weights provided, use them in the loss; convert to tensor later in setup\n","        self.register_buffer('class_weights_buf', torch.tensor(class_weights) if class_weights is not None else None)\n","\n","        # Metrics\n","        self.train_acc = Accuracy(task=\"multiclass\", num_classes=num_classes)\n","        self.val_acc = Accuracy(task=\"multiclass\", num_classes=num_classes)\n","        self.test_acc = Accuracy(task=\"multiclass\", num_classes=num_classes)\n","\n","    def _get_output_shape(self, shape):\n","        \"\"\"Return flattened size after conv layers\"\"\"\n","        batch_size = 1\n","        device = next(self.parameters()).device if any(p.requires_grad for p in self.parameters()) else 'cpu'\n","        x = torch.zeros(batch_size, *shape).to(device)\n","        with torch.no_grad():\n","            x = F.relu(self.conv1(x))\n","            x = self.pool1(F.relu(self.conv2(x)))\n","            x = F.relu(self.conv3(x))\n","            x = self.pool2(F.relu(self.conv4(x)))\n","        n_size = x.view(batch_size, -1).size(1)\n","        return n_size\n","\n","    def _feature_extractor(self, x):\n","        x = F.relu(self.conv1(x))\n","        x = self.pool1(F.relu(self.conv2(x)))\n","        x = F.relu(self.conv3(x))\n","        x = self.pool2(F.relu(self.conv4(x)))\n","        return x\n","\n","    def forward(self, x):\n","        x = self._feature_extractor(x)\n","        x = x.view(x.size(0), -1)\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        logits = self.fc3(x)  # return raw logits\n","        return logits\n","\n","    def _get_criterion(self):\n","        \"\"\"Create loss function, using class weights if available.\"\"\"\n","        if self.class_weights_buf is not None:\n","            return nn.CrossEntropyLoss(weight=self.class_weights_buf)\n","        return nn.CrossEntropyLoss()\n","\n","    def training_step(self, batch, batch_idx):\n","        x, y = batch\n","        logits = self(x)\n","        criterion = self._get_criterion()\n","        loss = criterion(logits, y)\n","        preds = torch.argmax(logits, dim=1)\n","        acc = self.train_acc(preds, y)\n","        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=False)\n","        self.log(\"train_acc\", acc, on_step=True, on_epoch=True, prog_bar=False)\n","        return loss\n","\n","    def validation_step(self, batch, batch_idx):\n","        x, y = batch\n","        logits = self(x)\n","        criterion = self._get_criterion()\n","        loss = criterion(logits, y)\n","        preds = torch.argmax(logits, dim=1)\n","        acc = self.val_acc(preds, y)\n","        self.log(\"val_loss\", loss, prog_bar=True, on_epoch=True)\n","        self.log(\"val_acc\", acc, prog_bar=True, on_epoch=True)\n","        return loss\n","\n","    def test_step(self, batch, batch_idx):\n","        x, y = batch\n","        logits = self(x)\n","        criterion = self._get_criterion()\n","        loss = criterion(logits, y)\n","        preds = torch.argmax(logits, dim=1)\n","        acc = self.test_acc(preds, y)\n","        self.log(\"test_loss\", loss, on_epoch=True)\n","        self.log(\"test_acc\", on_epoch=True)\n","        return loss\n","\n","    def configure_optimizers(self):\n","        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n","        return optimizer"],"metadata":{"id":"gaEV-nbef7VG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# GEPETO DATA LOADING\n","\n","fold_id = 0\n","\n","data_module = AlzheimerDataModule(\n","    df=df,\n","    fold_indices=fold_indices,\n","    fold_id=fold_id,\n","    batch_size=32,\n","    use_weighted_sampler=True\n",")\n","\n","# prepare_data is a no-op for this DM, but harmless if you want to keep the call\n","# data_module.prepare_data()\n","data_module.setup(\"fit\")  # creates train/val Subsets and computes class weights\n","\n","# ---- 1) Class weights and num_classes from the DataModule ----\n","# You no longer need to read labels from train_set; the DM already computed them.\n","class_weights = data_module.class_weights.cpu().numpy()  # ndarray[ num_classes ]\n","num_classes = data_module.num_classes\n","print(\"Class weights:\", class_weights)\n","\n","# ---- 2) Init model (RGB 128x128 as used in the DataModule transforms) ----\n","# Adjusted input_shape to (3, 128, 128) to match the transforms\n","model = AlzheimerLitModel(\n","    input_shape=(3, 128, 128),\n","    num_classes=num_classes,\n","    learning_rate=3e-4,\n","    class_weights=class_weights\n",")\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","\n","# ---- 3) Loss and optimizer ----\n","# Use the same weights buffer the model already registered\n","criterion = nn.CrossEntropyLoss(weight=model.class_weights_buf)\n","optimizer = optim.Adam(model.parameters(), lr=model.learning_rate)\n","\n","# ---- 4) Train loop (unchanged, just uses the DM loaders) ----\n","loss_values = []\n","eval_accuracy_values = []\n","num_epochs = 1 # Reduce epochs for faster testing\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","    running_loss = 0.0\n","    correct = 0\n","    total = 0\n","\n","    for inputs, targets in data_module.train_dataloader():\n","        inputs, targets = inputs.to(device), targets.to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, targets)\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item() * inputs.size(0)\n","        preds = torch.argmax(outputs, dim=1)\n","        correct += (preds == targets).sum().item()\n","        total += targets.size(0)\n","\n","    epoch_loss = running_loss / total\n","    epoch_acc = correct / total\n","    loss_values.append(epoch_loss)\n","    eval_accuracy_values.append(epoch_acc)\n","\n","    print(f\"Epoch [{epoch+1}/{num_epochs}] Loss: {epoch_loss:.4f} Accuracy: {epoch_acc:.4f}\")"],"metadata":{"id":"6NtJeSDUpviV"},"execution_count":null,"outputs":[]}]}