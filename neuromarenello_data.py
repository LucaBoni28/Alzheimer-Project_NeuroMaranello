# -*- coding: utf-8 -*-
"""NeuroMarenello_data.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PIgqH9xdhFgC8LEqQ5tFOnIQmdrjOkMC

# Milestone 1: Data Preparation

---

Installing the necessary modules
"""

!pip install pytorch-lightning --quiet
!pip install wandb --quiet
!pip install kagglehub --quiet

"""Importing the necessary libs"""

import pytorch_lightning as pl
import torch.nn.functional as F
import pytorch_lightning as pl
from torchmetrics.classification import Accuracy
from torch.utils.data import Dataset, DataLoader, Subset, WeightedRandomSampler
from torchvision import transforms
from torchvision.datasets import ImageFolder
from torch import nn
from torch.nn import functional as F
from torch.utils.data import DataLoader,random_split
from torchmetrics import Accuracy
import matplotlib.pyplot as plt
import os
from PIL import Image
import kagglehub
import cv2
from torch.utils.data import DataLoader, random_split
from pytorch_lightning.loggers import WandbLogger
import wandb
from sklearn.utils.class_weight import compute_class_weight
import torch.optim as optim
import torch

import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import StratifiedKFold

"""Downloading the Alzheimer's dataset from Kaggle (https://www.kaggle.com/datasets/marcopinamonti/alzheimer-mri-4-classes-dataset?resource=download)

And plotting one image for sanity check
"""

# Download the dataset
alzheimer_dataset = kagglehub.dataset_download("marcopinamonti/alzheimer-mri-4-classes-dataset")

# Obtaining the root folder path
root_path = os.path.join(alzheimer_dataset, os.listdir(alzheimer_dataset)[0])

# Obtaining the class list
class_list = os.listdir(root_path)

# Plotting one image of the first class for checking
class_path = os.path.join(root_path, class_list[0])
image = os.path.join(class_path, "1 (2).jpg")
plt.imshow(cv2.cvtColor(cv2.imread(image), cv2.COLOR_BGR2RGB))

"""Organizes and labels each image into its specific class and prepares data for the stratified K-Fold, which was chosen due to the unbalanced nature of the dataset"""

# Creating the rows of data by matching images with their labels
rows = []
for label in sorted(class_list):
    label_dir = os.path.join(root_path, label)
    for img in os.listdir(label_dir):
        rows.append({"class_path": os.path.join(label_dir, img), "label": label})

# Creating the data frame with sorted values by label, and reseting the index
df = pd.DataFrame(rows).sort_values("label").reset_index(drop=True)

# Encode labels 0..3 and converts the names of the classes into numbers
le = LabelEncoder()
df["y"] = le.fit_transform(df["label"])

print("Class â†’ index:", dict(zip(le.classes_, range(len(le.classes_)))))

"""Making Stratified K-Fold , which ensures that every fold has the same ratio of classes as the full dataset to make a fairer training and validation division."""

# We chose 3 folds
K = 3
X_idx = np.arange(len(df))
y = df["y"].to_numpy()

# Generating the 3 folds
skf = StratifiedKFold(n_splits=K, shuffle=True, random_state=70)

# Storing the validation and training indices for each fold
fold_indices = [(X_idx[tr], X_idx[va]) for tr, va in skf.split(X_idx, y)]
print(f"Prepared {len(fold_indices)} folds.")

"""Since our dataset is very unbalanced and we don't have enough images to test properly, we decided to divide the data into only training and validation sets. Below is the visualization of the 3 generated folds.


"""

def counts(arr):
    return dict(zip(le.classes_, np.bincount(arr, minlength=len(le.classes_))))

for i, (tr, va) in enumerate(fold_indices, start=1):
    print(f"\n=== Fold {i} ===")
    print("Train class counts:", counts(y[tr]))
    print("Val class counts:  ", counts(y[va]))

# Small dataset that reads from your data frame
class AlzheimerDataset(Dataset):
    def __init__(self, df, transform=None):
        self.df = df.reset_index(drop=True)
        self.transform = transform

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):     # Every time the dataloader gets a new image, it will access the dataframe with iloc and then this image will be transformed
        row = self.df.iloc[idx]
        img = Image.open(row["class_path"]).convert("RGB")
        if self.transform:
            img = self.transform(img)
        label = int(row["y"])
        return img, label     # The dataset will return the image and its label


class AlzheimerDataModule(pl.LightningDataModule):
    def __init__(
        self,
        df,
        fold_indices,
        fold_id=0,
        batch_size=32,
        use_weighted_sampler=True     # Due to the unbalanced dataset
    ):
        super().__init__()
        self.df = df.reset_index(drop=True)
        self.fold_indices = fold_indices
        self.fold_id = int(fold_id)
        self.batch_size = batch_size
        self.use_weighted_sampler = use_weighted_sampler

        # Makes sure all images are same size (128x128), transforms them to tensors and normalizes
        self.transform = transforms.Compose([
            transforms.Resize((128, 128)),
            transforms.ToTensor(),
            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
        ])

        # Gets the number of classes
        self.num_classes = int(self.df["y"].nunique())

    def setup(self, stage=None):
        train_idx, val_idx = self.fold_indices[self.fold_id]

        # Base dataset from df
        base_ds = AlzheimerDataset(self.df, transform=self.transform)

        # Creates a smaller dataset from the base dataset using our obtained training and validation indexes
        self.train_data = Subset(base_ds, train_idx)
        self.val_data   = Subset(base_ds, val_idx)

        # Computes the weights for each class based on the number of images, so the classes with less images have higher weight values
        train_labels = self.df.iloc[train_idx]["y"].to_numpy()
        classes = np.arange(self.num_classes)
        cw = compute_class_weight(class_weight='balanced', classes=classes, y=train_labels)

        # Increases the punishment of the model in case it fails to correctly label the class with less images, since we have less data to train on it
        self.class_weights = torch.tensor(cw, dtype=torch.float32)
        print(f"Class Weights: {self.class_weights.tolist()}")

        # We are applying a weighted sampler to increase the loading of the class with less images to try and make the training more balanced
        if self.use_weighted_sampler:
            class_counts = np.bincount(train_labels, minlength=self.num_classes)
            per_class_w = 1.0 / class_counts
            sample_w = per_class_w[train_labels]
            self.train_sampler = WeightedRandomSampler(
                weights=torch.tensor(sample_w, dtype=torch.double),
                num_samples=len(sample_w),
                replacement=True
            )
        else:
            self.train_sampler = None


    def train_dataloader(self):
        return DataLoader(
            self.train_data,
            batch_size=self.batch_size,
            shuffle=(self.train_sampler is None),     # In case there is no training sampler, we would shuffle the data
            sampler=self.train_sampler,               # However we are using a weighted sampler to try and increase the balance of the classes
            pin_memory=True                           # Optimizing the data transfer
        )

    def val_dataloader(self):
        return DataLoader(
            self.val_data,
            batch_size=self.batch_size,
            shuffle=False,
            pin_memory=True
        )
